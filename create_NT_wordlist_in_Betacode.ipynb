{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcc72d8-8cb0-4e16-810f-cb7885633467",
   "metadata": {},
   "source": [
    "# Greek word list in BetaCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36eecd4-7019-4fdd-b9a3-483941feb1be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of content (ToC)<a class=\"anchor\" id=\"TOC\"></a>\n",
    "* <a href=\"#bullet1\">1 - Create list of Greek words in Unicode</a>\n",
    "* <a href=\"#bullet2\">2 - Analyze Unicode accent storage</a>\n",
    "* <a href=\"#bullet3\">3 - Convert the word list into betacode</a>\n",
    "* <a href=\"#bullet4\">4 - Create a JSON dictionairy</a>\n",
    "* <a href=\"#bullet5\">5 - Atribution and footnotes</a>\n",
    "* <a href=\"#bullet6\">6 - Required libraries</a>\n",
    "* <a href=\"#bullet7\">7 - Notebook version</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3d5f7-c8aa-4b5e-9814-9bc8ce87479c",
   "metadata": {},
   "source": [
    "#  1 - Create list of Greek words in Unicode<a class=\"anchor\" id=\"bullet1\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The first step is to create a list of unique greek words in the New Testament. This will be otbained from the XML source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ef5ab2-f5fa-4a22-a19f-5a41fd97b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 XML files to process.\n",
      "Unique words saved to uniqueWords.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# GitHub repository details\n",
    "owner = \"tonyjurg\"\n",
    "repo = \"Nestle1904LFT\"\n",
    "branch = \"main\"\n",
    "path = \"resources/xml/20240210\"  # Input XML treebank for the Nestle 1904 Greek New Testament\n",
    "\n",
    "# Base URL for raw file content\n",
    "rawBaseUrl = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"\n",
    "\n",
    "# Option to use local files\n",
    "useLocal = True  # Set to False to fetch files from GitHub\n",
    "localInputDir = Path(\"C:/Users/tonyj/OneDrive/Documents/GitHub/grammarR-playground/XML-input\").resolve()\n",
    "outputFile = Path(\"uniqueWords.txt\")  # Output file for unique words\n",
    "\n",
    "def getRateLimit():\n",
    "    \"\"\"\n",
    "    Fetch and display the current GitHub API rate limit status.\n",
    "    \"\"\"\n",
    "    rateLimitUrl = \"https://api.github.com/rate_limit\"\n",
    "    response = requests.get(rateLimitUrl)\n",
    "    response.raise_for_status()\n",
    "    rateLimit = response.json()[\"rate\"]\n",
    "    print(f\"GitHub API Rate Limit: {rateLimit['remaining']} remaining out of {rateLimit['limit']} requests.\")\n",
    "\n",
    "def getFileList():\n",
    "    \"\"\"\n",
    "    Get the list of XML files either from the GitHub repository or from the local directory.\n",
    "    \"\"\"\n",
    "    if useLocal:\n",
    "        if not localInputDir.exists():\n",
    "            raise FileNotFoundError(f\"Local directory {localInputDir} does not exist.\")\n",
    "        return sorted(\n",
    "            file.name for file in localInputDir.glob(\"*.xml\") if re.match(r\"^\\d{2}-\", file.name)\n",
    "        )\n",
    "    else:\n",
    "        getRateLimit()\n",
    "        apiUrl = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
    "        response = requests.get(apiUrl)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()\n",
    "        return sorted(\n",
    "            file[\"name\"] for file in files if file[\"name\"].endswith(\".xml\") and re.match(r\"^\\d{2}-\", file[\"name\"])\n",
    "        )\n",
    "\n",
    "def processFile(fileName, uniqueWords):\n",
    "    \"\"\"\n",
    "    Parse and process the content of a single XML file to collect unique words.\n",
    "    \"\"\"\n",
    "    filePath = localInputDir / fileName if useLocal else f\"{rawBaseUrl}{fileName}\"\n",
    "    \n",
    "    if useLocal:\n",
    "        with filePath.open(\"rb\") as file:\n",
    "            xmlContent = file.read()\n",
    "    else:\n",
    "        response = requests.get(filePath)\n",
    "        response.raise_for_status()\n",
    "        xmlContent = response.content\n",
    "\n",
    "    # Parse the XML file\n",
    "    try:\n",
    "        root = ET.fromstring(xmlContent)  # Parse XML content from string\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fileName}: {e}\")\n",
    "        return  # Continue with other files\n",
    "\n",
    "    for word in root.findall(\".//w\"):\n",
    "        wordText = word.get(\"normalized\")  # Adjust the attribute if necessary\n",
    "        if wordText:\n",
    "            uniqueWords.add(wordText)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        fileNames = getFileList()\n",
    "        print(f\"Found {len(fileNames)} XML files to process.\")\n",
    "\n",
    "        uniqueWords = set()\n",
    "        for fileName in fileNames:\n",
    "            try:\n",
    "                processFile(fileName, uniqueWords)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {fileName}: {e}\")\n",
    "\n",
    "        # Write unique words to the output file\n",
    "        with outputFile.open(\"w\", encoding=\"utf-8\") as file:\n",
    "            for word in sorted(uniqueWords):  # Sort alphabetically before saving\n",
    "                file.write(word + '\\n')\n",
    "\n",
    "        print(f\"Unique words saved to {outputFile}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching file list or processing files: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e6a3a-0d25-47a8-a347-342d73dfe981",
   "metadata": {},
   "source": [
    "# 2 - Analyze Unicode accent storage<a class=\"anchor\" id=\"bullet2\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The distinction between pre-composed characters and separate accents in Unicode is essential for consistency in text processing, particularly in Greek, where accents convey grammatical and phonetic meaning. Pre-composed characters combine the base letter and accent into a single Unicode point, while separate accents use multiple code points. This difference can affect sorting, searching, and rendering, as systems may treat the two forms differently despite their identical appearance.\n",
    "\n",
    "The script reads Greek words from `uniqueWords.txt`, checks how their accents are stored, and categorizes them as pre-composed, separate accents, or mixed. Detailed output is stored to a JSON file (accentAnalysis.json), while a short summary is printed on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1ffccb-a574-4d0b-ab4c-fd20166bc1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomposed: 18480\n",
      "Separate accents: 0\n",
      "Mixed: 0\n",
      "Accent analysis saved to accentAnalysis.json.\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Path to the input file\n",
    "inputFile = 'uniqueWords.txt'\n",
    "\n",
    "# Function to check if a word uses pre-composed characters\n",
    "def checkAccentType(word):\n",
    "    \"\"\"\n",
    "    Determine if a word uses pre-composed characters or separate accent definitions.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The Greek word to check.\n",
    "\n",
    "    Returns:\n",
    "        str: \"precomposed\" if the word uses pre-composed characters,\n",
    "             \"separate accents\" if it uses separate accent definitions.\n",
    "    \"\"\"\n",
    "    normalizedNFC = unicodedata.normalize('NFC', word)  # Pre-composed form\n",
    "    normalizedNFD = unicodedata.normalize('NFD', word)  # Decomposed form\n",
    "\n",
    "    if word == normalizedNFC:\n",
    "        return \"precomposed\"\n",
    "    elif word == normalizedNFD:\n",
    "        return \"separate accents\"\n",
    "    else:\n",
    "        return \"mixed\"\n",
    "\n",
    "# Read Greek words from the input file\n",
    "with open(inputFile, 'r', encoding='utf-8') as inFile:\n",
    "    greekWords = inFile.read().splitlines()\n",
    "\n",
    "# Analyze each word for accent storage\n",
    "accentAnalysis = {word: checkAccentType(word) for word in greekWords}\n",
    "\n",
    "# Print results\n",
    "precomposedCount = sum(1 for v in accentAnalysis.values() if v == \"precomposed\")\n",
    "separateAccentsCount = sum(1 for v in accentAnalysis.values() if v == \"separate accents\")\n",
    "mixedCount = sum(1 for v in accentAnalysis.values() if v == \"mixed\")\n",
    "\n",
    "print(f\"Precomposed: {precomposedCount}\")\n",
    "print(f\"Separate accents: {separateAccentsCount}\")\n",
    "print(f\"Mixed: {mixedCount}\")\n",
    "\n",
    "# Save the results to a file\n",
    "outputFile = 'accentAnalysis.json'\n",
    "import json\n",
    "with open(outputFile, 'w', encoding='utf-8') as outFile:\n",
    "    json.dump(accentAnalysis, outFile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Accent analysis saved to {outputFile}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c32b8-a420-443d-bb9b-7593bff0ec48",
   "metadata": {},
   "source": [
    "# 3 - Convert the word list into betacode<a class=\"anchor\" id=\"bullet3\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "This script converts the previous list of Greek words stored in `uniqueWords.txt` into its coresponding Beta Code, a transliteration system for Greek used as input to Morpheus morphological tagger.\n",
    "\n",
    "The script reads the Greek words, applies the beta_code.greek_to_beta_code function (found on GitHub repository [perseids-tools/beta-code-py](https://github.com/perseids-tools/beta-code-py)) to convert each word, and writes the Beta Code equivalents to `betaCodeWords.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f640916-fd49-408e-bd74-2846e4d0bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18480 words to Beta Code and saved to betaCodeWords.txt.\n"
     ]
    }
   ],
   "source": [
    "import beta_code\n",
    "\n",
    "def capitalizeIfAllCaps(word):\n",
    "    if word.isupper():  # Check if the word is all uppercase\n",
    "        return word.capitalize()  # Capitalize only the first letter\n",
    "    return word  # Leave the word unchanged if it's not all uppercase\n",
    "    \n",
    "\n",
    "# Paths to input and output files\n",
    "inputFile = 'uniqueWords.txt'       # File containing Greek Unicode words\n",
    "outputFile = 'betaCodeWords.txt'    # File to save the converted Beta Code words\n",
    "\n",
    "# Read Greek words from the input file\n",
    "with open(inputFile, 'r', encoding='utf-8') as inFile:\n",
    "    greekWords = inFile.read().splitlines()\n",
    "\n",
    "# Convert each Greek word to Beta Code\n",
    "betaCodeWords = [beta_code.greek_to_beta_code(capitalizeIfAllCaps(word)) for word in greekWords]\n",
    "\n",
    "# Write the Beta Code words to the output file\n",
    "with open(outputFile, 'w', encoding='utf-8') as outFile:\n",
    "    for word in betaCodeWords:\n",
    "        outFile.write(word + '\\n')\n",
    "\n",
    "print(f\"Converted {len(greekWords)} words to Beta Code and saved to {outputFile}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b68c06-7fc2-4522-a4b0-0ee03356095d",
   "metadata": {},
   "source": [
    "# 4 - Create a JSON dictionairy<a class=\"anchor\" id=\"bullet4\"></a>\n",
    "##### [Back to ToC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f59aa-56da-480a-b209-55428345eaeb",
   "metadata": {},
   "source": [
    "The following script creates a JSON file where the Greek words are the keys and their corresponding Beta Code representations are the values. This dictionairy assists in tranlating back the results from the Morpheus lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18bcb102-7f16-4e5f-a495-79920766edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created JSON file with 18480 entries: betaCodeToWord.json\n"
     ]
    }
   ],
   "source": [
    "import beta_code\n",
    "import json\n",
    "\n",
    "def capitalizeIfAllCaps(word):\n",
    "    if word.isupper():  # Check if the word is all uppercase\n",
    "        return word.capitalize()  # Capitalize only the first letter\n",
    "    return word  # Leave the word unchanged if it's not all uppercase\n",
    "\n",
    "# Paths to input and output files\n",
    "inputFile = 'uniqueWords.txt'       # File containing Greek Unicode words\n",
    "outputFile = 'betaCodeToWord.json'   # File to save the Greek-to-Beta Code mapping\n",
    "\n",
    "# Read Greek words from the input file\n",
    "with open(inputFile, 'r', encoding='utf-8') as inFile:\n",
    "    greekWords = inFile.read().splitlines()\n",
    "\n",
    "# Create a dictionary with Greek words as keys and Beta Code as values\n",
    "wordsBetaCodeMap = {beta_code.greek_to_beta_code(capitalizeIfAllCaps(word)): word for word in greekWords}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(outputFile, 'w', encoding='utf-8') as outFile:\n",
    "    json.dump(wordsBetaCodeMap, outFile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Created JSON file with {len(wordsBetaCodeMap)} entries: {outputFile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596174b1-316d-45de-87fe-11fbc3f8b551",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5 - Footnotes and attribution<a class=\"anchor\" id=\"bullet5\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The engine of the conversion is provided by the `beta-code-py` library found on GitHub repository [perseids-tools/beta-code-py](https://github.com/perseids-tools/beta-code-py) available under MIT license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a400b-81c9-4ba1-b240-81fe9e31b46d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6 - Required libraries<a class=\"anchor\" id=\"bullet6\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The scripts in this notebook require the following Python libraries to be installed in the environment:\n",
    "\n",
    "    beta_code \n",
    "    json\n",
    "    os  \n",
    "    pathlib\n",
    "    re\n",
    "    requests\n",
    "    unicodedata\n",
    "    xml\n",
    "\n",
    "You can install any missing library from within Jupyter Notebook using either`pip` or `pip3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca64b7-bd35-42fb-8e19-2cfaf74addc4",
   "metadata": {},
   "source": [
    "# 7 - Notebook version<a class=\"anchor\" id=\"bullet7\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "<div style=\"float: left;\">\n",
    "  <table>\n",
    "    <tr>\n",
    "      <td><strong>Author</strong></td>\n",
    "      <td>Tony Jurg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Version</strong></td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Date</strong></td>\n",
    "      <td>2 December 2024</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
